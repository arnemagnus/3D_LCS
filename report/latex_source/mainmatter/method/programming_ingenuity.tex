\section[Making the most of the available computational resources]
{Making the most of the available computational \\\phantom{3.12} resources}
\label{sec:making_the_most_of_the_available_computational_resources}

The simultaneous solution of twelve coupled ODEs involved in the advection of
tracers in order to compute the flow map and its directional derivatives
(cf.\ \cref{sec:computing_the_flow_map_and_its_directional_derivatives})
quickly proved an unreasonably strenuous task for the author's own personal
laptop. Seeing as the available memory was the main limitation, we parallelized
this computation by means of MPI, and ran it on NTNU's supercomputer, Vilje.
In spite of the tracers being independent, we elected to utilize MPI over
alternative multiprocessing tools in order to access multiple nodes within
the Vilje cluster. Due to the problem's pleasingly parallel nature,
the parallelization process consisted of distributing an approximately even
amount of tracers across all ranks, whereupon each rank advected (that is,
simultaneously solved the twelve coupled ODEs for all of) its allocated
tracers. In the end, all of the final state flow map Jacobians were collected
by the designated main process (i.e.,\ $\text{rank}=0$), whereupon the
Cauchy-Green strain eigenvalues and -vectors were extracted by means of an SVD
decomposition (as outlined in \cref{sec:computing_cauchy_green_strain%
_eigenvalues_and_vectors}).

Regarding the generation of manifolds, code profiling (unsurprisingly) revealed
that the generation of new mesh points by computing (quasi-)radial
trajectories orthogonal to the $\vct{\xi}_{3}$-direction field was a great
source of time expenditure. Accordingly, we rewrote all numerical routines
pertaining to the generation of new mesh points in Cython. In particular, we
made use of highly optimized, low-level BLAS\footnote{See
\url{www.netlib.org/blas} and
\url{https://docs.scipy.org/doc/scipy/reference/linalg.cython_blas.html}}
routines whenever possible. Because of the ever increasing number of necessary
triangle comparisons in our algorithm of detectng manifolds which
self-intersect (as described in
\cref{sub:continuous_self_intersection_checks}), all of the accompanying
numerical methods were expressed in Cython. Similarly to how we exposed the
Bspline-Fortran library to Python (cf.\
\cref{sub:interpolating_gridded_velocity_data}), we also consistently used
calls by reference in order to avoid unneccesary memory duplication. The
transition from (NumPy-based) Python to Cython for the most significant
tasks reduced the overall runtime by two orders of magnitude --- on top of the
the reduction by (up to) two orders of magnitude from utilizing the revised
rather than the legacy approach to computing mesh points (mentioned in
\cref{sec:revised_approach_to_computing_new_mesh_points}).

Analogously to the advection of tracers, we made use of the mutual independence
of the computed manifolds to accelerate their computation by means of MPI
parallelization across the Vilje cluster. We elected to make a one-to-one
correspondence between the number of MPI threads and the number of manifolds to
generate (as described in
\cref{sub:identifying_suitable_initial_conditions_for_developing_lcss}) such
that each manifold was allotted as much working memory as possible,
facilitating each manifold to grow as large as possible before being stopped
due to the one of the criteria proposed in \cref{sec:macroscale_stopping%
_criteria_for_the_expansion_of_computed_manifolds}. Compared to the advection
of tracers, or the expansion of manifolds, the extraction of repelling LCSs as
subsets of the computed manifolds (cf.\ \cref{sec:identifying_lcss_as_subsets%
_of_computed_manifolds}) was not a particularly laborious task. Thus, we chose
to parallelize this selection process by making use of the Python
\texttt{multiprocessing} library, as access to a single node in the Vilje
cluster sufficed to complete it in less than a minute.
