\section[Regarding our approach to computing the Cauchy-Green
strain characteristics] {Regarding our approach to computing the%
    \\\phantom{5.1} Cauchy-Green strain characteristics}
\label{sec:regarding_our_approach_to_computing_the_cauchy_green%
_strain_characteristics}

We used an SVD decomposition of the flow map Jacobian to find the Cauchy-Green
strain eigenvalues and -vectors, rather than computing these directly from the
Cauchy-Green strain tensor field --- as described in
\cref{sec:computing_the_flow_map_and_its_directional_derivatives,%
sec:computing_cauchy_green_strain_eigenvalues_and_vectors}. This approach,
suggested by \textcite{miron2012anisotropic} and endorsed by
\textcite{oettinger2016autonomous}, boasts superior accuracy compared to the
more conventional approach of approximating the directional derivatives of the
flow map (i.e.,\ the components of the flow map Jacobian) by applying a finite
difference method and then explicitly computing the Cauchy-Green strain
tensor field (which \textcite{farazmand2012computing} did in order to find LCSs
in two-dimensional flow). In addition to the increased mathematical complexity
of directly transporting the flow map Jacobian field, our approach relies on
bounded first spatial derivatives of the underlying velocity field, as is
evident from inspecting \cref{eq:timederivative_flowmap_jacobian}. This should,
however, not be an issue when considering smooth analytical test cases, or when
using a high (quadratic or higher, cf.\
\cref{sub:spline_interpolation_of_discrete_data}) order interpolation method for
gridded data. Alternatively, the derivatives can be approximated by e.g.\ a
finite difference method. Should any of these approaches prove impractical, the
method of \textcite{farazmand2012computing} could be sufficient.

Note that the resolution of the grid of tracers, on which the Cauchy-Green
strain eigenvalues and -vectors are computed (see
\cref{sec:computing_the_flow_map_and_its_directional_derivatives,%
sec:computing_cauchy_green_strain_eigenvalues_and_vectors}), plays a critically
important role in the successful detection of LCSs. For instance, we were
unable to reproduce the spherical LCS of \cref{sub:an_analytical_lcs_test_case}
using (quite significantly) fewer tracers; which naturally resulted in a
reduced number of initial conditions satisfying LCS existence conditions~
\eqref{eq:lcs_condition_a},~\eqref{eq:lcs_condition_b}
and~\eqref{eq:lcs_condition_d} (see
\cref{sub:identifying_suitable_initial_conditions_for_developing_lcss}).
Difficulties arose with such a sparse grid of tracers that no initial
conditions for the generation of manifolds layed sufficiently close to the
strongly repulsive unit sphere (see \cref{fig:spherical_lm3}). To our
knowledge, there is no way to determine \emph{a priori} what density of tracers
will suffice for any flow system. Thus, even though educated guesses based on
the scale at which one is interested in the microscopic behaviour in the system
might be prudent, we recommend to use as fine a grid of tracers as possible,
within the constraints set by the available computational resources.
based on the scale at which one is interested in the

As mentioned in \cref{sec:flow_systems_defined_by_gridded_velocity_data}, we
interpolated the model velocity field using quadrivariate, cubic B-splines ---
that is, cubic interpolation in time and all spatial direction. This involved
us having to keep the model data pertaining to the region of interest (that is,
the model data for a domain expanding beyond said region in all directions, in
order to resolve the behaviour near the boundaries, cf.\
\cref{sec:macroscale_stopping_criteria_for_the_expansion_of_computed_manifolds})
for the entirety of the considered time interval. Because of our data set's
disparate resolution in the horizontal and vertical directions (which is
outlined in \cref{sec:flow_systems_defined_by_gridded_velocity_data}), in
addition to the small spatial region of interest (in comparison to the entire
fjord), the use of quadrivariate interpolation was unproblematic regarding
the consumption of working memory. For other applications, however, this is not
necessarily the case, depending on the problem's scale (temporal and spatial)
and the resolution of the model data.

Should memory consumption be an issue for a discrete dataset, it is possible
to forego temporal interpolation entirely (provided that the sampling rate
is adequate), and instead opt for trivariate interpolation in space, generating
an interpolation object for each time instance. This renders the use of
ODE solvers of adaptive stepsize --- like the Dormand-Prince 8(7) method we
wound up choosing (more on that to follow in
\cref{sec:on_the_choice_of_numerical_ode_solver}) --- moot, as the solution
time steps would then have to coincide with the time levels of the dataset.
Using a lower order ODE solution method, such as the explicit trapezoidal rule
(which does not require intermediary samples when moving from one time level to
the next), yields inferior performance; high order embedded Runge-Kutta solvers
are much less prone to numerical error \parencite{loken2017sensitivity}.



